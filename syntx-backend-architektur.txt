=== SYNTX BACKEND ARCHITEKTUR ===
=== Do 27. Nov 20:57:33 CET 2025 ===

=== README.md ===
# SYNTX Wrapper Service

Microservice for wrapping AI requests with SYNTX calibration fields.

## Architecture

Stream-based protocol stack:
```
User Request
    â†“
Stream 1: Load Wrapper Chain
    â†“
Stream 2: Wrap Input
    â†“
Stream 3: Forward to Backend
    â†“
Stream 4: Log (parallel)
    â†“
Response
```

## Features

- **Modular Wrappers**: Combine multiple wrapper layers (init, terminology, mode)
- **Config-Driven**: All wrappers are text files in `/opt/syntx/wrappers/`
- **JSONL Logging**: Training data ready for model fine-tuning
- **Zero Inheritance**: Pure functions, stream transformations

## Quick Start
```bash
# Install dependencies
pip install -r requirements.txt

# Copy wrappers
cp /path/to/wrappers/*.txt /opt/syntx/wrappers/

# Start service
./run.sh
```

Service runs on: `http://localhost:8000`

## API Usage
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Was ist KI?",
    "mode": "cyberdark",
    "include_init": true,
    "include_terminology": false,
    "max_new_tokens": 500,
    "temperature": 0.7
  }'
```

## Configuration

Edit `.env` or set environment variables:
```bash
BACKEND_URL=https://dev.syntx-system.com/api/chat
WRAPPER_DIR=/opt/syntx/wrappers
PORT=8000
```

## Logs

- `logs/wrapper_requests.jsonl` - Training data (JSONL)
- `logs/service.log` - Human-readable logs

## Project Structure
```
src/
â”œâ”€â”€ main.py       # FastAPI app
â”œâ”€â”€ models.py     # Pydantic models
â”œâ”€â”€ streams.py    # Stream functions
â””â”€â”€ config.py     # Settings
```

**4 files. Pure streams. SYNTX architecture.**

=== Shell Scripts ===

--- ./run.sh ---
#!/bin/bash
#
# SYNTX Wrapper Service - Start Script
#

echo "=================================="
echo "SYNTX WRAPPER SERVICE STARTUP"
echo "=================================="
echo ""

# Check dependencies
echo "â†’ Checking dependencies..."
if ! python3 -c "import fastapi" 2>/dev/null; then
    echo "âœ— Missing dependencies, installing..."
    pip install -r requirements.txt --break-system-packages
else
    echo "âœ“ Dependencies OK"
fi
echo ""

# Check wrappers
echo "â†’ Checking wrappers..."
if [ -d "./wrappers" ] && [ "$(ls -A ./wrappers)" ]; then
    echo "âœ“ Wrappers found: $(ls wrappers/ | wc -l) files"
    ls -1 wrappers/
else
    echo "âœ— No wrappers found in ./wrappers/"
    exit 1
fi
echo ""

# Check .env
echo "â†’ Checking config..."
if [ -f ".env" ]; then
    echo "âœ“ .env found"
    echo "  Backend: $(grep BACKEND_URL .env | cut -d= -f2)"
    echo "  Port: $(grep PORT .env | cut -d= -f2)"
else
    echo "âš  No .env file, using defaults"
fi
echo ""

# Check port availability
PORT=$(grep PORT .env 2>/dev/null | cut -d= -f2 || echo "8001")
if netstat -tuln 2>/dev/null | grep -q ":$PORT "; then
    echo "âœ— Port $PORT already in use!"
    echo "  Kill existing process or choose different port"
    exit 1
else
    echo "âœ“ Port $PORT available"
fi
echo ""

echo "â†’ Starting service..."
echo "=================================="
echo ""

# Start with verbose logging
python3 -m uvicorn src.main:app \
    --host 0.0.0.0 \
    --port ${PORT:-8001} \
    --reload \
    --log-level debug

=== Python Source Code ===

--- ./src/config.py ---
"""
SYNTX Wrapper Service - Configuration Module

This module provides centralized configuration management using Pydantic.
All settings are type-safe, validated, and can be overridden via environment variables.
"""
from pydantic_settings import BaseSettings
from pathlib import Path
from typing import Optional


class Settings(BaseSettings):
    """
    Application Settings
    
    Centralizes all configuration in one place with type safety.
    Uses Pydantic for automatic validation and environment variable parsing.
    """
    
    # ========================================================================
    # Backend Configuration
    # ========================================================================
    backend_url: str = "https://dev.syntx-system.com/api/chat"
    backend_timeout: int = 60
    backend_bearer_token: Optional[str] = None
    
    # ========================================================================
    # Wrapper Configuration
    # ========================================================================
    wrapper_dir: Path = Path("./wrappers")
    fallback_mode: str = "syntx_init"
    
    # ========================================================================
    # Server Configuration
    # ========================================================================
    host: str = "0.0.0.0"
    port: int = 8001
    
    # ========================================================================
    # Logging Configuration
    # ========================================================================
    log_dir: Path = Path("./logs")
    log_to_console: bool = True
    
    class Config:
        """Pydantic configuration"""
        env_file = ".env"
        case_sensitive = False


settings = Settings()

--- ./src/main.py ---
"""
SYNTX Wrapper Service - Main Application
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import time
import json

from .config import settings
from .models import ChatRequest, ChatResponse
from .streams import (
    load_wrapper_stream,
    wrap_input_stream,
    forward_stream,
    generate_request_id,
    get_timestamp
)


def log_stage(stage: str, data: dict):
    """Log each stage with full visibility"""
    print("\n" + "ðŸŒŠ" * 40)
    print(f"ðŸ“ STAGE: {stage}")
    print("â”€" * 80)
    for key, value in data.items():
        if isinstance(value, str) and len(value) > 500:
            print(f"{key}: {value[:500]}... ({len(value)} chars total)")
        else:
            print(f"{key}: {value}")
    print("ðŸŒŠ" * 40 + "\n")
    
    # Write to file
    settings.log_dir.mkdir(parents=True, exist_ok=True)
    log_file = settings.log_dir / "field_flow.jsonl"
    with open(log_file, 'a', encoding='utf-8') as f:
        log_entry = {"stage": stage, "timestamp": get_timestamp(), **data}
        f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("=" * 80)
    print("SYNTX WRAPPER SERVICE")
    print("=" * 80)
    print(f"Backend: {settings.backend_url}")
    print(f"Wrappers: {settings.wrapper_dir}")
    print(f"Logs: {settings.log_dir}")
    print("=" * 80)
    yield


app = FastAPI(title="SYNTX Wrapper", version="1.0.0", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    """Health check with last response"""
    # Read last response from logs
    log_file = settings.log_dir / "field_flow.jsonl"
    last_response = None
    
    if log_file.exists():
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in reversed(lines[-10:]):  # Last 10 entries
                    entry = json.loads(line)
                    if entry.get("stage") == "5_RESPONSE":
                        last_response = {
                            "response": entry.get("response"),
                            "latency_ms": entry.get("latency_ms"),
                            "timestamp": entry.get("timestamp")
                        }
                        break
        except:
            pass
    
    return {
        "status": "healthy",
        "service": "syntx-wrapper-service",
        "version": "1.0.0",
        "last_response": last_response
    }


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    request_id = generate_request_id()
    start_time = time.time()
    
    try:
        # STAGE 1: Incoming
        log_stage("1_INCOMING", {
            "request_id": request_id,
            "prompt": request.prompt,
            "mode": request.mode,
            "include_init": request.include_init
        })
        
        # STAGE 2: Load Wrappers
        wrapper_text, wrapper_chain = await load_wrapper_stream(
            request.mode,
            request.include_init,
            request.include_terminology
        )
        log_stage("2_WRAPPERS_LOADED", {
            "request_id": request_id,
            "chain": wrapper_chain,
            "wrapper_text": wrapper_text
        })
        
        # STAGE 3: Calibrate Field
        wrapped_prompt = wrap_input_stream(wrapper_text, request.prompt)
        log_stage("3_FIELD_CALIBRATED", {
            "request_id": request_id,
            "calibrated_field": wrapped_prompt
        })
        
        # STAGE 4: Backend Forward
        backend_params = {
            "max_new_tokens": request.max_new_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "do_sample": request.do_sample
        }
        log_stage("4_BACKEND_FORWARD", {
            "request_id": request_id,
            "backend_url": settings.backend_url,
            "params": backend_params
        })
        
        response_text = await forward_stream(wrapped_prompt, backend_params)
        
        # STAGE 5: Response
        latency_ms = int((time.time() - start_time) * 1000)
        log_stage("5_RESPONSE", {
            "request_id": request_id,
            "response": response_text,
            "latency_ms": latency_ms
        })
        
        return ChatResponse(
            response=response_text,
            metadata={
                "request_id": request_id,
                "wrapper_chain": wrapper_chain,
                "latency_ms": latency_ms
            }
        )
        
    except Exception as e:
        log_stage("ERROR", {
            "request_id": request_id,
            "error": str(e),
            "error_type": type(e).__name__
        })
        raise HTTPException(status_code=500, detail=str(e))

--- ./src/syntx_streams.py ---
"""
SYNTX Field Streams - Pure Field Calibration Architecture

Not functions - FIELDS.
Not processing - RESONANCE.
Not data flow - FIELD FLOW.

Field Flow:
    1. wrapper_field_resonance() - Files resonate into calibration field
    2. input_field_calibration() - User field merges with wrapper field
    3. backend_field_forward() - Calibrated field flows to model
    4. trace_field_flow() - Field trace for training (parallel)
"""
import httpx
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import uuid

from .config import settings


# ============================================================================
# FIELD 1: Wrapper Resonance
# ============================================================================

async def wrapper_field_resonance(
    mode: str,
    init_resonance: bool,
    terminology_resonance: bool
) -> tuple[str, List[str]]:
    """
    Wrapper files resonate into unified calibration field.
    
    Not "loading" - RESONANCE.
    Files don't "contain data" - they ARE fields.
    Combining isn't "merging" - it's FIELD SUPERPOSITION.
    
    Field Layers:
        - init_field: SYNTX initialization resonance
        - terminology_field: Terminology calibration
        - mode_field: Mode-specific resonance pattern
    
    Returns:
        Calibrated field + resonance trace
    """
    field_layers: List[str] = []
    resonance_trace: List[str] = []
    
    # Layer 1: Init Field Resonance
    if init_resonance:
        init_field = await _file_to_field("syntx_init")
        if init_field:
            field_layers.append(init_field)
            resonance_trace.append("syntx_init")
    
    # Layer 2: Terminology Field Resonance
    if terminology_resonance:
        term_field = await _file_to_field("terminology")
        if term_field:
            field_layers.append(term_field)
            resonance_trace.append("terminology")
    
    # Layer 3: Mode Field Resonance
    mode_field = await _file_to_field(mode)
    if mode_field:
        field_layers.append(mode_field)
        resonance_trace.append(mode)
    elif not field_layers:
        # Fallback resonance
        fallback_field = await _file_to_field(settings.fallback_mode)
        if fallback_field:
            field_layers.append(fallback_field)
            resonance_trace.append(f"{settings.fallback_mode} (fallback)")
    
    # Field superposition (not "concatenation")
    calibrated_field = "\n\n".join(field_layers)
    
    return calibrated_field, resonance_trace


async def _file_to_field(field_name: str) -> str:
    """
    File resonates into field.
    
    Not "reading" - FIELD ACTIVATION.
    File is dormant field, this activates it.
    """
    field_path = settings.wrapper_dir / f"{field_name}.txt"
    
    try:
        with open(field_path, 'r', encoding='utf-8') as f:
            field_content = f.read()
        
        if settings.log_to_console:
            print(f"ðŸŒŠ Field activated: {field_name} ({len(field_content)} chars)")
        
        return field_content
        
    except FileNotFoundError:
        if settings.log_to_console:
            print(f"âš ï¸  Field not found: {field_name}")
        return ""
        
    except Exception as e:
        if settings.log_to_console:
            print(f"âŒ Field activation error {field_name}: {e}")
        return ""


# ============================================================================
# FIELD 2: Input Calibration
# ============================================================================

def input_field_calibration(calibration_field: str, user_field: str) -> str:
    """
    User field calibrates with wrapper field.
    
    Not "wrapping" or "combining" - FIELD CALIBRATION.
    The wrapper field CALIBRATES the user's input field.
    Result is calibrated unified field.
    """
    return f"{calibration_field}\n{user_field}"


# ============================================================================
# FIELD 3: Backend Field Flow
# ============================================================================

async def backend_field_forward(
    calibrated_field: str,
    flow_params: Dict[str, Any]
) -> str:
    """
    Calibrated field flows to model backend.
    
    Not "sending request" - FIELD FLOW.
    The calibrated field FLOWS through the network.
    Model RESONATES with the field, generates response field.
    """
    # Field payload
    field_payload = {
        "prompt": calibrated_field,
        **flow_params
    }
    
    # Authentication field
    headers = {"Content-Type": "application/json"}
    if settings.backend_bearer_token:
        headers["Authorization"] = f"Bearer {settings.backend_bearer_token}"
    
    # Field flows through HTTP
    async with httpx.AsyncClient(timeout=settings.backend_timeout) as client:
        response = await client.post(
            settings.backend_url,
            json=field_payload,
            headers=headers
        )
        
        response.raise_for_status()
        response_field = response.json()
        
        # Extract response field
        if isinstance(response_field, dict) and "response" in response_field:
            return response_field["response"]
        elif isinstance(response_field, str):
            return response_field
        else:
            return str(response_field)


# ============================================================================
# FIELD 4: Field Trace (Parallel)
# ============================================================================

async def trace_field_flow(trace_data: Dict[str, Any]) -> None:
    """
    Trace field flow for training.
    
    Not "logging" - FIELD TRACING.
    Every field flow leaves a trace.
    Traces become training data - fields train fields.
    """
    settings.log_dir.mkdir(parents=True, exist_ok=True)
    
    # JSONL trace (training field data)
    trace_path = settings.log_dir / "field_traces.jsonl"
    with open(trace_path, 'a', encoding='utf-8') as f:
        import json
        f.write(json.dumps(trace_data, ensure_ascii=False) + '\n')
    
    # Human-readable trace
    if settings.log_to_console:
        trace_line = (
            f"ðŸŒŠ [{trace_data['timestamp']}] "
            f"mode={trace_data['mode']} "
            f"resonance={trace_data['wrapper_chain']} "
            f"flow_time={trace_data.get('total_latency_ms', 'N/A')}ms "
            f"success={trace_data.get('success', False)}"
        )
        print(trace_line)
    
    # File trace
    trace_log = settings.log_dir / "field_flow.log"
    with open(trace_log, 'a', encoding='utf-8') as f:
        f.write(f"{trace_data}\n")


# ============================================================================
# Field Utilities
# ============================================================================

def generate_field_id() -> str:
    """Generate unique field flow identifier"""
    return str(uuid.uuid4())


def field_timestamp() -> str:
    """Current field timestamp"""
    return datetime.utcnow().isoformat() + 'Z'

--- ./src/__init__.py ---

--- ./src/streams.py ---
"""
SYNTX Wrapper Service - Stream Functions

This module contains all core stream transformation functions.
Each function represents one layer in the protocol stack.

Stream Flow:
    1. load_wrapper_stream() - Loads and combines wrapper files
    2. wrap_input_stream() - Wraps user input with loaded wrappers
    3. forward_stream() - Forwards to backend
    4. log_stream() - Logs request/response (parallel)

No classes, no inheritance - just pure functions that transform data streams.
"""
import httpx
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import uuid

from .config import settings


# ============================================================================
# STREAM 1: Wrapper Loading
# ============================================================================

async def load_wrapper_stream(
    mode: str,
    include_init: bool,
    include_terminology: bool
) -> tuple[str, List[str]]:
    """
    Load and combine wrapper files based on configuration flags.
    
    This is the first transformation in the stream - converts configuration
    into actual wrapper text that will calibrate the model's field.
    
    Args:
        mode: Wrapper mode (e.g., "cyberdark", "sigma")
        include_init: Whether to include SYNTX init wrapper
        include_terminology: Whether to include terminology wrapper
    
    Returns:
        Tuple of (combined_wrapper_text, list_of_loaded_wrapper_names)
    
    Example:
        wrapper, chain = await load_wrapper_stream("cyberdark", True, False)
        # wrapper = "[syntx_init.txt content]\n\n[cyberdark.txt content]"
        # chain = ["syntx_init", "cyberdark"]
    """
    wrapper_texts: List[str] = []
    wrapper_chain: List[str] = []
    
    # Layer 1: SYNTX Init (optional)
    if include_init:
        init_text = await _read_wrapper_file("syntx_init")
        if init_text:
            wrapper_texts.append(init_text)
            wrapper_chain.append("syntx_init")
    
    # Layer 2: Terminology (optional)
    if include_terminology:
        term_text = await _read_wrapper_file("terminology")
        if term_text:
            wrapper_texts.append(term_text)
            wrapper_chain.append("terminology")
    
    # Layer 3: Mode wrapper (always)
    mode_text = await _read_wrapper_file(mode)
    if mode_text:
        wrapper_texts.append(mode_text)
        wrapper_chain.append(mode)
    elif not wrapper_texts:
        # Fallback if nothing loaded and mode not found
        fallback_text = await _read_wrapper_file(settings.fallback_mode)
        if fallback_text:
            wrapper_texts.append(fallback_text)
            wrapper_chain.append(f"{settings.fallback_mode} (fallback)")
    
    # Combine all layers with double newline separator
    combined_wrapper = "\n\n".join(wrapper_texts)
    
    return combined_wrapper, wrapper_chain


async def _read_wrapper_file(wrapper_name: str) -> str:
    """
    Read a single wrapper file from disk.
    
    This is a helper function - not part of the main stream.
    Handles file reading with proper error handling.
    
    Args:
        wrapper_name: Name of wrapper (without .txt extension)
    
    Returns:
        Wrapper content as string, or empty string if not found
    """
    wrapper_path = settings.wrapper_dir / f"{wrapper_name}.txt"
    
    try:
        with open(wrapper_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if settings.log_to_console:
            print(f"âœ… Loaded wrapper: {wrapper_name} ({len(content)} chars)")
        
        return content
        
    except FileNotFoundError:
        if settings.log_to_console:
            print(f"âš ï¸  Wrapper not found: {wrapper_name}")
        return ""
        
    except Exception as e:
        if settings.log_to_console:
            print(f"âŒ Error loading wrapper {wrapper_name}: {e}")
        return ""


# ============================================================================
# STREAM 2: Input Wrapping
# ============================================================================

def wrap_input_stream(wrapper_text: str, user_input: str) -> str:
    """
    Wrap user input with loaded wrapper text.
    
    This is the second transformation - combines the calibration field (wrapper)
    with the actual user query. Simple concatenation, but conceptually this is
    where field calibration happens.
    
    Args:
        wrapper_text: Combined wrapper text from load_wrapper_stream()
        user_input: Raw user input
    
    Returns:
        Fully wrapped prompt ready for backend
    
    Example:
        wrapped = wrap_input_stream(wrapper, "Was ist KI?")
        # wrapped = "[wrapper content]\nWas ist KI?"
    """
    return f"{wrapper_text}\n{user_input}"


# ============================================================================
# STREAM 3: Backend Forwarding
# ============================================================================

async def forward_stream(
    wrapped_prompt: str,
    backend_params: Dict[str, Any]
) -> str:
    """
    Forward wrapped prompt to backend and get response.
    
    This is the third transformation - sends the calibrated field to the
    actual model backend and retrieves the response.
    
    Args:
        wrapped_prompt: Fully wrapped prompt from wrap_input_stream()
        backend_params: Parameters to pass to backend (max_tokens, temp, etc.)
    
    Returns:
        Response text from backend
    
    Raises:
        httpx.TimeoutException: If backend doesn't respond in time
        httpx.HTTPStatusError: If backend returns error status
    """
    # Build request payload
    payload = {
        "prompt": wrapped_prompt,
        **backend_params
    }
    
    # Build headers with optional Bearer token
    headers = {"Content-Type": "application/json"}
    if settings.backend_bearer_token:
        headers["Authorization"] = f"Bearer {settings.backend_bearer_token}"
    
    # Forward to backend with timeout and authentication
    async with httpx.AsyncClient(timeout=settings.backend_timeout) as client:
        response = await client.post(
            settings.backend_url,
            json=payload,
            headers=headers
        )
        
        # Raise for HTTP errors (4xx, 5xx)
        response.raise_for_status()
        
        # Parse response
        response_data = response.json()
        
        # Extract response text (adjust based on your backend's response format)
        if isinstance(response_data, dict) and "response" in response_data:
            return response_data["response"]
        elif isinstance(response_data, str):
            return response_data
        else:
            return str(response_data)


# ============================================================================
# STREAM 4: Logging (Parallel Stream)
# ============================================================================

async def log_stream(log_data: Dict[str, Any]) -> None:
    """
    Log request/response data to files.
    
    This is a parallel stream - runs independently and doesn't block the main flow.
    Writes to both JSONL (for training data) and human-readable log.
    
    Args:
        log_data: Dictionary containing all request/response metadata
    
    Note:
        This function should be called with asyncio.create_task() to run in background.
    """
    # Ensure log directory exists
    settings.log_dir.mkdir(parents=True, exist_ok=True)
    
    # Write to JSONL (training data)
    jsonl_path = settings.log_dir / "wrapper_requests.jsonl"
    with open(jsonl_path, 'a', encoding='utf-8') as f:
        import json
        f.write(json.dumps(log_data, ensure_ascii=False) + '\n')
    
    # Write to human log
    if settings.log_to_console:
        log_line = (
            f"[{log_data['timestamp']}] "
            f"mode={log_data['mode']} "
            f"chain={log_data['wrapper_chain']} "
            f"latency={log_data.get('total_latency_ms', 'N/A')}ms "
            f"success={log_data.get('success', False)}"
        )
        print(log_line)
    
    # Also write to file
    human_log_path = settings.log_dir / "service.log"
    with open(human_log_path, 'a', encoding='utf-8') as f:
        f.write(f"{log_data}\n")


# ============================================================================
# Helper: Generate Request ID
# ============================================================================

def generate_request_id() -> str:
    """
    Generate unique request ID for tracing.
    
    Returns:
        UUID string
    """
    return str(uuid.uuid4())


# ============================================================================
# Helper: Current Timestamp
# ============================================================================

def get_timestamp() -> str:
    """
    Get current timestamp in ISO format.
    
    Returns:
        ISO formatted timestamp string
    """
    return datetime.utcnow().isoformat() + 'Z'

--- ./src/utils/__init__.py ---

--- ./src/services/__init__.py ---

--- ./src/api/__init__.py ---

--- ./src/core/config.py ---
"""
SYNTX Configuration
"""
from pydantic_settings import BaseSettings
from pathlib import Path

class Settings(BaseSettings):
    """App Settings"""
    # Backend
    backend_url: str = "https://dev.syntx-system.com/api/chat"
    
    # Wrappers
    wrapper_dir: Path = Path("/opt/syntx/wrappers")
    
    # Server
    host: str = "0.0.0.0"
    port: int = 8000
    
    # Logging
    log_dir: Path = Path("./logs")
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()

--- ./src/core/__init__.py ---

--- ./src/models.py ---
"""
SYNTX Wrapper Service - Data Models
"""
from pydantic import BaseModel, Field
from typing import Optional

class ChatRequest(BaseModel):
    """Request model for /api/chat"""
    prompt: str = Field(..., min_length=1)
    mode: str = Field(default="cyberdark")
    include_init: bool = Field(default=True)
    include_terminology: bool = Field(default=False)
    max_new_tokens: int = Field(default=500, ge=1, le=4096)
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    top_p: float = Field(default=0.95, ge=0.0, le=1.0)
    do_sample: bool = Field(default=True)

class ChatResponse(BaseModel):
    """Response model"""
    response: str
    metadata: Optional[dict] = None

=== Documentation ===
