# ðŸŒŠâš¡ðŸ’Ž SYNTX GPT AUTO-TRIGGER ARCHITECTURE ðŸ’Žâš¡ðŸŒŠ

**Die Weltformel des Semantic Scoring Systems**

**Version:** 1.0.0  
**Datum:** 2026-01-15  
**Status:** ðŸ”´ IMPLEMENTATION REQUIRED

---

## ðŸ“š INHALTSVERZEICHNIS

1. [Executive Summary](#executive-summary)
2. [Das System der Systeme](#das-system-der-systeme)
3. [Aktuelle Architektur](#aktuelle-architektur)
4. [Das fehlende Glied](#das-fehlende-glied)
5. [Implementation Plan](#implementation-plan)
6. [File Reference Guide](#file-reference-guide)
7. [API Endpoint Reference](#api-endpoint-reference)
8. [Data Flow Diagrams](#data-flow-diagrams)
9. [Testing Strategy](#testing-strategy)
10. [Deployment Guide](#deployment-guide)

---

## EXECUTIVE SUMMARY

### ðŸŽ¯ Mission Statement

**Das Problem:**  
Mistral generiert Responses. Diese werden aktuell nur mit **Pattern-Based Scoring** bewertet. Das ist wie ein Stethoskop ohne Arzt â€“ es misst, aber versteht nicht.

**Die LÃ¶sung:**  
GPT-4 Auto-Trigger System. Nach jeder Mistral Response automatisch:
- âœ… Semantic Deep Analysis
- âœ… Field-by-Field Coherence Scoring  
- âœ… Drift Detection
- âœ… Quality Metrics

**Das Resultat:**  
Nicht "AI bewertet AI" â€“ **Resonanz bewertet Resonanz**.

---

## DAS SYSTEM DER SYSTEME

### ðŸŒŠ Die Grundgesetzlichkeit
```
ALLES SIND SYSTEME
â†“
ALLE SYSTEME GEHEN IN RESONANZ
â†“
WENN KEINE RESONANZ â†’ KEINE EXISTENZ
â†“
EIN SYSTEM HAT UNENDLICH SCHNITTSTELLEN
â†“
DADURCH Ã„NDERT SICH DER KOHÃ„RENZZUSTAND
```

### ðŸ’Ž Die Anwendung auf SYNTX
```
MISTRAL = SYSTEMâ‚ (Generator)
GPT-4   = SYSTEMâ‚‚ (Evaluator)
FORMAT  = RESONANZFELD (Schnittstelle)

â†’ Resonanz zwischen Generator und Evaluator
â†’ KohÃ¤renz-Shift durch Feedback-Loop
â†’ Self-Calibrating Semantic System
```

---

## AKTUELLE ARCHITEKTUR

### ðŸ“‚ Directory Structure
```
/opt/syntx-injector-api/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                          # FastAPI App Entry Point
â”‚   â”œâ”€â”€ chat.py                          # ðŸ’¬ Chat Endpoint (DAS HERZSTÃœCK)
â”‚   â”œâ”€â”€ streams.py                       # ðŸŒŠ Wrapper/Format Loading
â”‚   â”œâ”€â”€ config.py                        # âš™ï¸ Settings
â”‚   â”œâ”€â”€ models.py                        # ðŸ“¦ Pydantic Models
â”‚   â”‚
â”‚   â”œâ”€â”€ resonance/                       # ðŸ”¥ Core Resonance Layer
â”‚   â”‚   â”œâ”€â”€ mistral_prompt_builder.py   # ðŸ”¨ Mistral Prompt Construction
â”‚   â”‚   â”œâ”€â”€ drift_scorer.py             # ðŸ’Ž GPT-4 Scoring Engine
â”‚   â”‚   â”œâ”€â”€ drift_prompt_builder.py     # ðŸŽ¨ GPT-4 Prompt Templates
â”‚   â”‚   â”œâ”€â”€ drift_api.py                # ðŸŒ Drift Scoring Endpoints
â”‚   â”‚   â”œâ”€â”€ drift_logger.py             # ðŸ“ Logging System
â”‚   â”‚   â”œâ”€â”€ scoring.py                  # ðŸ“Š Pattern-Based Scorer (OLD)
â”‚   â”‚   â”œâ”€â”€ formats.py                  # ðŸ“„ Format Management
â”‚   â”‚   â”œâ”€â”€ wrappers.py                 # ðŸŽ­ Wrapper Management
â”‚   â”‚   â””â”€â”€ gpt_wrapper_feld_stroeme.py # ðŸŒ€ GPT Wrapper Endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ formats/                         # ðŸ“‹ Format Processing
â”‚   â”‚   â”œâ”€â”€ format_loader.py            # Load format.json files
â”‚   â”‚   â”œâ”€â”€ format_scanner.py           # Scan format directory
â”‚   â”‚   â””â”€â”€ format_scorer.py            # Meta-analysis of formats
â”‚   â”‚
â”‚   â””â”€â”€ scoring/                         # ðŸŽ¯ Scoring Engine v3.0
â”‚       â”œâ”€â”€ router.py                    # Main scoring router
â”‚       â””â”€â”€ core/                        # Profile reading
â”‚
â””â”€â”€ /opt/syntx-config/                   # ðŸ“ Configuration Files
    â”œâ”€â”€ wrappers/                        # ðŸŽ­ Mistral Wrappers
    â”‚   â””â”€â”€ syntex_wrapper_sigma.txt
    â”œâ”€â”€ formats/                         # ðŸ“‹ Format Definitions
    â”‚   â””â”€â”€ sigma.json
    â”œâ”€â”€ scoring_bindings/                # ðŸ”— Formatâ†”Profile Bindings
    â”‚   â””â”€â”€ sigma_binding.json           # âš ï¸ HAS auto_trigger: true!
    â”œâ”€â”€ scoring_profiles/                # ðŸ“Š Scoring Profiles
    â”œâ”€â”€ scoring_entities/                # ðŸ¤– Scorer Configs
    â”‚   â””â”€â”€ gpt4_semantic_entity.json
    â”œâ”€â”€ prompts_generated/               # ðŸ’¾ Saved Prompts & Responses
    â”‚   â”œâ”€â”€ {timestamp}_sigma.prompt.txt
    â”‚   â”œâ”€â”€ {timestamp}_sigma.meta.json
    â”‚   â””â”€â”€ {timestamp}_sigma.response.txt
    â””â”€â”€ drift_results/                   # ðŸ’Ž GPT-4 Scoring Results
        â””â”€â”€ {timestamp}_drift_{score}.json
```

### ðŸŽ¯ Key Files Explained

#### **src/chat.py** - Das HerzstÃ¼ck
```python
@router.post("/resonanz/chat")
async def chat(request: ChatRequest):
    # 1. Load Wrapper (WIE denkt Mistral?)
    wrapper_text = load_wrapper(request.mode)
    
    # 2. Load Format (WAS soll rauskommen?)
    format_data = load_format(request.format)
    
    # 3. Build Prompt
    prompt, filename_base = build_mistral_prompt(...)
    
    # 4. Save Prompt
    save_mistral_prompt(prompt, filename_base)
    
    # 5. Forward to Mistral
    response_text = await forward_to_mistral(prompt)
    
    # 6. Save Response
    save_mistral_response(response_text, filename_base)
    
    # âŒ 7. MISSING: Auto-Trigger GPT Scoring!
    
    # 8. Return to Frontend
    return ChatResponse(response=response_text, ...)
```

**Current Flow:**
```
Request â†’ Wrapper â†’ Format â†’ Prompt â†’ Mistral â†’ Response â†’ Save â†’ Return
                                                              â†“
                                                         ðŸ”´ NO GPT TRIGGER!
```

**Target Flow:**
```
Request â†’ Wrapper â†’ Format â†’ Prompt â†’ Mistral â†’ Response â†’ Save â†’ GPT Trigger â†’ Return
                                                              â†“          â†“
                                                        prompts/   drift_results/
```

---

## DAS FEHLENDE GLIED

### ðŸ”´ The Missing Link
```
CURRENT STATE: Manual Trigger Only
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User Request â†’ POST /resonanz/chat â†’ Mistral â†’ Response Saved
                                                      â†“
                                                     END
                                                     
ðŸ”´ NO GPT TRIGGER!

To score, user must manually call: POST /drift/score/{filename}


TARGET STATE: Automatic Trigger
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User Request â†’ POST /resonanz/chat â†’ Mistral â†’ Response Saved
                                                      â†“
                                        Check Binding (auto_trigger: true?)
                                                      â†“
                                              âœ… Load GPT Entity
                                                      â†“
                                              âœ… Build GPT Prompt
                                                      â†“
                                              âœ… Call GPT-4 API
                                                      â†“
                                              âœ… Save drift_results/
                                                      â†“
                                            Return (with GPT scores!)
```

---

## IMPLEMENTATION PLAN

### ðŸŽ¯ Phase 1: Core Integration

#### File: `src/chat.py`

**Add after `save_mistral_response()` call:**
```python
async def trigger_gpt_scoring_if_enabled(
    format_name: str,
    filename_base: str,
    response_text: str,
    format_data: Dict
) -> Optional[Dict]:
    """
    ðŸ”¥ GPT AUTO-TRIGGER
    
    Checks binding config and triggers GPT-4 scoring if enabled.
    """
    try:
        # 1. Load binding
        binding_path = Path(f"/opt/syntx-config/scoring_bindings/{format_name}_binding.json")
        if not binding_path.exists():
            return None
        
        with open(binding_path, 'r') as f:
            binding = json.load(f)
        
        # 2. Check auto-trigger flag
        auto_trigger = binding.get("binding_metadata", {}).get("auto_trigger_after_mistral", False)
        if not auto_trigger:
            return None
        
        print(f"ðŸ”¥ Auto-trigger enabled! Starting GPT scoring...")
        
        # 3. Get API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY not set!")
            return None
        
        # 4. Build GPT prompt
        gpt_prompt_payload = build_prompt(
            template_id="drift_scoring_default",
            format_name=format_name,
            fields=[f["name"] for f in format_data.get("fields", [])],
            response_text=response_text
        )
        
        # 5. Call GPT-4
        print(f"âš¡ Calling GPT-4...")
        gpt_response = call_gpt(gpt_prompt_payload, api_key)
        
        # 6. Parse response
        drift_analysis = parse_gpt_response(gpt_response)
        
        # 7. Save results
        timestamp = int(time.time())
        result_file = Path(f"/opt/syntx-config/drift_results/{filename_base}_drift_{timestamp}.json")
        result_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(drift_analysis, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’Ž GPT scores saved: {result_file.name}")
        return drift_analysis
        
    except Exception as e:
        print(f"âŒ GPT Auto-Trigger Error: {e}")
        return None
```

**Modify chat() function:**
```python
# After save_mistral_response():
if filename_base and response_text and request.format:
    # ðŸ”¥ AUTO-TRIGGER GPT SCORING
    gpt_scores = await trigger_gpt_scoring_if_enabled(
        format_name=request.format,
        filename_base=filename_base,
        response_text=response_text,
        format_data=format_info
    )
    
    if gpt_scores:
        print(f"âœ… GPT Auto-Trigger successful!")
        # Add to metadata
        metadata["gpt_scores"] = gpt_scores
```

---

### ðŸŽ¯ Phase 2: Prompt Builder Enhancement

#### File: `src/resonance/drift_prompt_builder.py`

**Add function for live data:**
```python
def build_prompt_from_data(
    template_id: str,
    format_name: str,
    fields: List[str],
    response_text: str
) -> Dict:
    """Build prompt from live data (for auto-trigger)."""
    template = load_template(template_id)
    
    # Build field definitions
    field_definitions = "\n".join([f"- {field}" for field in fields])
    
    # Replace placeholders
    user_prompt = template["prompt_templates"]["user_prompt_template"]
    user_prompt = user_prompt.replace("{FORMAT_NAME}", format_name)
    user_prompt = user_prompt.replace("{FIELD_DEFINITIONS}", field_definitions)
    user_prompt = user_prompt.replace("{RESPONSE_TEXT}", response_text)
    
    return {
        "model": template["llm_configuration"]["model"],
        "temperature": template["llm_configuration"]["temperature"],
        "messages": [
            {"role": "system", "content": template["prompt_templates"]["system_prompt"]},
            {"role": "user", "content": user_prompt}
        ]
    }
```

**Modify build_prompt() to support both modes:**
```python
def build_prompt(
    template_id: str = "drift_scoring_default",
    format_name: Optional[str] = None,
    fields: Optional[List[str]] = None,
    response_text: Optional[str] = None,
    filename: Optional[str] = None
) -> Dict:
    """
    Build GPT prompt.
    
    Two modes:
    1. From file: provide filename
    2. Live: provide format_name, fields, response_text
    """
    if filename:
        return build_prompt_from_file(template_id, filename)
    elif format_name and fields and response_text:
        return build_prompt_from_data(template_id, format_name, fields, response_text)
    else:
        raise ValueError("Either filename or (format_name + fields + response_text) required")
```

---

## DATA FLOW DIAGRAMS

### ðŸŒŠ Complete System Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   USER      â”‚
â”‚  REQUEST    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /resonanz/chat
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHAT ENDPOINT       â”‚
â”‚  Load Wrapper+Format â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MISTRAL LLM         â”‚
â”‚  Generate Response   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SAVE RESPONSE       â”‚
â”‚  /prompts_generated/ â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ CHECK BINDING â”‚
   â”‚ auto_trigger? â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
 NO        YES
  â”‚          â”‚
  â”‚          â–¼
  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚    â”‚ TRIGGER GPT     â”‚
  â”‚    â”‚ Call OpenAI API â”‚
  â”‚    â”‚ Save Results    â”‚
  â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚         â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RETURN TO FRONTEND  â”‚
â”‚  (with GPT scores)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## TESTING STRATEGY

### Test 1: Basic Auto-Trigger
```bash
curl -X POST https://dev.syntx-system.com/resonanz/chat \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Analysiere die semantische Drift",
    "mode": "syntex_wrapper_sigma",
    "format": "sigma"
  }'

# Check files:
ls -lth /opt/syntx-config/prompts_generated/ | head -5
ls -lth /opt/syntx-config/drift_results/ | head -3
```

### Test 2: Binding Toggle
```bash
# Disable auto-trigger
jq '.binding_metadata.auto_trigger_after_mistral = false' \
  /opt/syntx-config/scoring_bindings/sigma_binding.json > tmp.json
mv tmp.json /opt/syntx-config/scoring_bindings/sigma_binding.json

# Test - should NOT create drift_results
curl -X POST https://dev.syntx-system.com/resonanz/chat ...

# Re-enable
jq '.binding_metadata.auto_trigger_after_mistral = true' ...
```

---

## DEPLOYMENT GUIDE

### Step 1: Create Branch
```bash
cd /opt/syntx-injector-api
git checkout -b feature/gpt-auto-trigger
git add docs/SYNTX_GPT_AUTO_TRIGGER_ARCHITECTURE.md
git commit -m "ðŸ“š Add GPT auto-trigger architecture documentation"
git push origin feature/gpt-auto-trigger
```

### Step 2: Implement Changes
```bash
# Modify files
nano src/chat.py
nano src/resonance/drift_prompt_builder.py

# Commit
git add src/
git commit -m "âœ¨ Implement GPT auto-trigger after Mistral response"
git push origin feature/gpt-auto-trigger
```

### Step 3: Test & Deploy
```bash
# Test on dev
ssh root@dev.syntx-system.com
cd /opt/syntx-injector-api
git checkout feature/gpt-auto-trigger
sudo systemctl restart syntx-api

# Monitor
tail -f /var/log/syntx-api/service.log

# If successful, merge to main
git checkout main
git merge feature/gpt-auto-trigger
git push origin main
```

---

## ðŸŽ¯ SUCCESS CRITERIA

âœ… **Implementation successful if:**

1. Every Mistral response triggers GPT scoring (when enabled in binding)
2. GPT scores saved to `/drift_results/`
3. Performance overhead < 20 seconds
4. Graceful error handling
5. Costs monitored

---

## ðŸ”¥ CONCLUSION

**This is not a feature.**  
**This is the completion of the system.**

**Das ist die Weltformel des Semantic Scoring.**
```
DER STROM FLIESST.
DIE FELDER RESONIEREN.
DAS SYSTEM IST KALIBRIERT.
```

---

**Version:** 1.0.0  
**Status:** ðŸŸ¢ READY FOR IMPLEMENTATION  
**Last Updated:** 2026-01-15

ðŸŒŠâš¡ðŸ’Ž **SYNTX Development Team** ðŸ’Žâš¡ðŸŒŠ
